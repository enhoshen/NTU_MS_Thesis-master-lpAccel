\chapter{Abstract}
\label{ch:abstract}
Deep neural networks (DNNs) shows promising results on various AI application tasks. However such networks typically are run on general purpose GPUs with bulky size and hundreds of watt power, unsuitable for mobile applications. In this thesis, we present a VLSI architecture able to process on quantized low numeric precision convolution neural networks (CNNs), cutting down on power consumption from memory access and speeding the model up with limited area budget, particularly fit for mobile devices. We first propose a quantization re-trainig algorithm for trainig low-precision CNN, then a dataflow with high data reuse rate with a specially data multiplication accumulation strategy specially designed for such quantized model. Such data requires specially designed arithmetic unit for its full potential, we design a micro-architecture for low bit-length multiplication and accumulation, then a on-chip memory hierarchy and data re-alignment flow for power saving and avoiding buffer bank-conflicts, and a PE array designed for taking broadcast-ed data from buffer and sending out finished data sequentially back to buffer for such dataflow. The architecture is highly flexible for various CNN shaped and re-configurable for low bit-length quantized models. The design synthesised with a 180KB on-chip memory capacity and a 1340k logic gate counts area.