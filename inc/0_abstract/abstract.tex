\chapter{Abstract}
\label{ch:abstract}
Deep neural networks (DNNs) shows promising results on various AI application tasks. However such networks typically are executed on general purpose GPUs with bulky size in form factor and hundreds of watt in power consumption, which is unsuitable for mobile applications. In this thesis, we present a VLSI architecture able to execute quantized low numeric-precision convolution neural networks (CNNs), cutting down power consumption from memory access and speeding up the model with limited area budget, which particularly fits for mobile devices. We first propose a quantization re-training algorithm for training low-precision CNN, as well as a dataflow with high data reuse rate with a specially data multiplication accumulation strategy specially designed for such quantized model. To fully utilize the efficiency of computation with such lowprecision data, we also design a micro-architecture for low bit-length multiplication and accumulation, an on-chip memory hierarchy and data realignment flow for power saving and avoiding buffer bank-conflicts, and a processing element (PE) array designed for taking broadcast-ed data from buffer and sending out result data sequentially back to the buffer for such dataflow. The architecture is highly flexible for various CNN shapes and re-configurable for low bit-length quantized models. We have implemented the proposed VLSI architecture with TSMC 90nm cell library. with the hardware cost of 180KB onchip memory and 1340k logic gate counts, the implementation result shows stateof-the-art hardware efficiency.
