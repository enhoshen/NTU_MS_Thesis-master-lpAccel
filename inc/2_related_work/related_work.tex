\chapter{Related Work}
\label{ch:related_work}
Many approaches have been proposed to help with deploying DNN on mobile device, either in algorithm or hardward aspect. In this chapter, we will discuss more about the main optimization scheme we choose, which is quantisation, and then existing hardware-designed DNN accelerators.
\section{Quantisation}
Quantization maps data to a smaller set of quantization levels. The principle is to minimize the error between the reconstructed data from the quantized one and the original data. The quantization level essentially reflects the bits required to represent the data. Reduced data bits comes with several benefits includes reduced storage costs, memory transactions and computational cost. There are ways to quantization, from simplest uniform distance between each quantization level to special mapping function such as \textit{log} function so that distance between quantization steps are of logarithm relation; and even more drastic approaches like clustering data into groups with k-means, requiring look up table for the mapping and computation. The simplest quantization also known as \textit{linear quantization} related closely to this project, and we're going to focus on it particularly. \TODO{quantization figure}
\subsection{fixed point quantisation}
More related to this work, researches find the numerical requirement for even the latest DNN model inference stage far from the commonly used 32/64 bit floating-point format. \cite{FixedPoint} quantize the models to fixed point with ${L_2}$ error minimization, achieved even better accuracy on MNIST and CIFAR-10. 
\subsection{ternary to binary quantisation}
Some researchers have tried extreme quantization down to 2bits tenary weights such as in TWNs\cite{Ternary} and even binary weights in BinaryNet \cite{BinaryNet}, to binarize both weight and activation as in XNOR-net\cite{XnorNet}. \\
TWNs minimizes the Euclidian distance between the full precision weights $\boldsymbol{W}$ and tenary-valued weights $\boldsymbol{W^t}$ along with a non-negative scaling factor $\alpha$ in \eqref{eq:twn} achieving \textbf{99.35\%}, \textbf{92.56\%}, \textbf{84.2\%} on \textbf{MNIST}, \textbf{CIFAR-10}, \textbf{ImageNet(top-5)} dataset respectively.  
\begin{equation}
\begin{aligned}\label{eq:twn}
    \alpha^*, \boldsymbol{W^{t*}} = \mathop{\arg\min}_{\alpha,\boldsymbol{W^t}} = \|\boldsymbol{W}-\alpha\boldsymbol{W^t}\|^2_2 \\  
\text{s.t }\alpha\geq0,\boldsymbol{W^t_i}\in\{1,0,-1\}, i=1,2,...,n.
\end{aligned}
\end{equation}
BinaryNet 


\subsection{8-bit quantization on modern models}
\section{Hardware design}
\subsection{}

\begin{itemize}
    \item \textcolor{purple}{How does related work "relate" to your research question?}
    \item \textcolor{purple}{Does it provide a starting point?}
    \item \textcolor{purple}{Does it provide evidence that you are posing a good research question?}
    \item \textcolor{purple}{Does it provide a different approach to a similar research hypothesis that you can compare to?}
\end{itemize}